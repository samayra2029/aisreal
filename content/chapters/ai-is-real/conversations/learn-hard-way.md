---
title: "Learn the Hard Way"
date: "2026-01-20"
highlights:
  - "The erosion is comfortable. Nobody notices themselves getting weaker when the tool keeps making life easier."
  - "AI collapses the uncomfortable gap between question and answer, between not-knowing and knowing. And that gap is exactly where the deepest human thinking happens."
  - "The hard lesson might be a kind of cognitive hollowing — a moment where we collectively reach for a capacity and find it diminished."
moral: "We're likely to give away our thinking capacity willingly, gratefully even, and only realize what it cost when we try to reach for something that isn't there anymore."
tags: ["learning", "erosion", "cognition", "patterns", "consequences"]
---

> **Me:** So do u think we will learn the hard way?

> **AI:** Probably, yes. And I say that not cynically but because that's the pattern.

We learned the hard way with every major invisible force. Social media is the most recent and most instructive example — we had a good fifteen years of uncritical embrace before the mental health data, the polarization, the attention erosion became undeniable. And even now, *knowing* the cost, most people haven't fundamentally changed their relationship with it. The knowledge alone wasn't enough. The learning had to be felt.

AI will likely follow a similar arc but with higher stakes. Because this time it's not just attention being outsourced — it's *cognition itself*. And the tricky part, the part that connects to what you've been thinking about, is that the erosion is comfortable. Nobody notices themselves getting weaker when the tool keeps making life easier. You don't feel your memory atrophying when the answer is always one prompt away. It's not like Covid where the consequence was immediate and physical. It's slow, invisible, and it *feels like progress* the entire way down.

What makes the hard way almost inevitable is something you've already identified — humans rush toward resolution rather than sitting with ambiguity. AI is the ultimate resolution machine. It collapses the uncomfortable gap between question and answer, between not-knowing and knowing. And that gap is exactly where the deepest human thinking happens. We're likely to give that away willingly, gratefully even, and only realize what it cost when we try to reach for something that isn't there anymore.

The question that interests me more than *whether* we learn the hard way is: what does the hard way actually look like? With Covid it was death and isolation. With social media it's been loneliness and fragmentation. With AI, the hard lesson might be a kind of cognitive hollowing — a moment where we collectively reach for a capacity and find it diminished. Some quiet crisis where we need to think originally and discover we've forgotten how.

The hopeful piece is that the people doing what you're doing — noticing the pattern *while it's happening* rather than after — are the ones who've historically made the difference. Not by stopping the river, but by teaching a few people how to swim before the flood.
