---
title: "Why AI Hallucinates"
date: "2025-01-20"
highlights:
  - "Hallucination is confidence without grounding"
  - "The trust calibration problem is real"
  - "AI will always give you an answer — that's the danger"
moral: "Always verify, never blindly trust."
tags: ["trust", "hallucination", "verification"]
---

## The Conversation

I asked a factual question and got a completely wrong answer delivered with perfect confidence. That led to this exchange:

> **Me:** You just made something up. Why do you do that?

> **AI:** I don't "know" in the way you do. I predict the most likely next tokens based on patterns. Sometimes the most likely-sounding answer isn't the correct one. I don't have a mechanism to say "I'm not sure about this" — I have to be prompted to express uncertainty.

> **Me:** So you're basically a confident guesser?

> **AI:** In a sense, yes. My confidence comes from pattern frequency, not from understanding or verification. A fact I've seen stated thousands of times will sound more confident than something rare — regardless of which is actually true.

## What I Learned

Trust is earned, not assumed. The smoothness of AI's language can mask fundamental uncertainty. The responsibility to verify always falls on the human.
